tasa.cambio <- read.csv('tasaDecambio.csv', header = FALSE)
#head(tasa.cambio)
tasa.cambio <- ts(tasa.cambio$V2, frequency = 12, start = c(1992,11))
tasa.desocupacion <- read.csv('tasadesocupacion.csv', header = FALSE)
#head(tasa.desocupacion)
tasa.desocupacion <- ts(tasa.desocupacion$V2, frequency = 12, start = 2005)
salarios <- read.csv('salariosreales.csv', header = FALSE)
#head(salarios)
salarios <- ts( salarios$V2, frequency = 12, start=2000)
datos <- ts.intersect(inpc, m.o, remesas, salarios,
tasa.cambio, tasa.desocupacion)
par(mfrow = c(2, 3))
for(i in 1:6)
{
ts.plot(datos[, i], col=colores[i], main=
as.character(colnames(datos))[i], xlab='',
ylab='',  lwd=2)
}
par(mfrow = c(1, 1))
#library(TSstudio) graficas del estilo plotly
#ts_plot(datos, type='multiple')
#ts_info(datos)
###################################################
# Chunk 14: noestacionalidad
###################################################
# estacionalizar las variables lo mejor que se pueda
# el detalle va a ser que se pierden observaciones por
# las diferenciaciones
############################################
series <- colnames(datos)
series.estacionales <- list()
round(apply(datos, 2, function(x)adf.test(x)$p.value),2)
# Chunk 16
#al encontrar una transformacion prudente la guardabamos
#prudente <- diff(diff(serie, lag= 1), lag=12)
#adf.test(prudente) #no hay raiz
#ts.plot(prudente, main=series[j])
#acf(prudente)
#pacf(prudente)
# aqui las hicimos explicitas para ahorrar tiempo de computo
series.estacionales <- cbind(diff(datos[, 1], 1), diff(diff(datos[,2], 12), 1),
diff(datos[, 3], 11), diff(diff(datos[,4], 12), 1),
diff(datos[, 5], 1), diff(diff(datos[, 6], 12)))
# inpc:solo 1
# m.0 : 1x12
# remesas lag 11
# salarios 1x12
# tasa.cambio 1
# tasa.desocupacion 1x12
datos.estacionales <- series.estacionales
colnames(datos.estacionales) <- series
meta.ts <- ts.intersect(datos.estacionales[,"inpc"], datos.estacionales[,"m.o"],
datos.estacionales[,'remesas'], datos.estacionales[,'salarios'],
datos.estacionales[,'tasa.cambio'],
datos.estacionales[,'tasa.desocupacion']   )
colnames(meta.ts) <- series
meta.ts <- na.omit(meta.ts)
# Chunk 17: estacionalidad.pruebas
for( i in 1:dim(meta.ts)[2])
{
print( series[i])
print(adf.test(meta.ts[,i])$p.value)
acf(meta.ts[, i], main=series[i])
pacf(meta.ts[, i], main=series[i])
}
# Chunk 18: estacionalidad
#################Graficamos
# el metodo facil
#############
par(mfrow = c(2, 3))
for(i in 1:6)
{
ts.plot(meta.ts[, i], col=colores[i],
main=paste0(as.character(colnames(datos))[i], ' estacionalizado'),
xlab='',  ylab='',  lwd=2)
}
par(mfrow = c(1, 1))
# Chunk 19: estimacion1
####################
# Estimacion y suavecimiento
##################
muestra <- meta.ts
set.seed(0)
predic.VAR.p <- predict.VAR.init(ensamble=muestra)
M.predic.VAR <- mapply(predic.VAR.p, 1:125)
AI <- as.data.frame(t(M.predic.VAR))
AI$n <- 1:125
library(ggplot2)
ggplot(AI, aes(x=n ,y=inpc, color=I(colores[2])))+
xlab('n obs. más nuevas usadas en el conjunto de entrenamiento')+
geom_line()+ theme_minimal() +ylab('SMAPE') +
ggtitle('Error en % al pronosticar el INP')
#apply(AI, 2, mean, na.rm=TRUE)
#apply(AI, 2, sd, na.rm=TRUE)
#a <- which.min(AI$inpc)
#a
#AI[a,]
pronostico1 <- modelo1(historico = muestra)
rezago <- pronostico1$rezago
a <- pronostico1$pronostico$fcst$inpc[1]
pronostico1 <- a + inpc[length(inpc)-rezago]
# Chunk 20
ts.plot(inpc, col = 'purple',  xlab='', ylab='',
main='INPC no estacional') #varianza no constante
ts.plot(inpc, col = 'purple',xlim=c(1994, 1998),
main='Zoom alrededor de 1995', xlab='', ylab='') #varianza no constante
# Chunk 21
Busetti.Harvey(as.numeric(inpc), option = 'c', posicion =315, k=1)
# Chunk 22
sub.inpc <- subset(inpc, start=315, end=571)
sub.m.o <- subset(m.o, start =138, end = 394 )
sub.remesas <- subset(remesas, start =16 , end = 272 )
sub.tasa.cambio <- subset(tasa.cambio, start = 55, end = 311)
par(mfrow=c(1,1))
##################################################
datos2 <- ts.intersect(sub.inpc, sub.m.o, sub.remesas,
sub.tasa.cambio)
par(mfrow = c(2, 2))
for(i in 1:4)
{
ts.plot(datos2[, i], col=colores[i], main=
as.character(colnames(datos2))[i], xlab='',
ylab='',  lwd=2)
}
par(mfrow = c(1, 1))
# Chunk 23
adf.test(sub.inpc)
adf.test(sub.m.o)
adf.test(sub.remesas)
adf.test(sub.tasa.cambio)
# Chunk 24
d.sub.inpc <- diff(sub.inpc, lag=1)
d.sub.m.o <- diff(diff(sub.m.o, 1), 12)
d.sub.remesas <- diff(diff(sub.remesas, lag=12), 1)
d.sub.tasa.cambio <- diff(sub.tasa.cambio, 4)
datos.sub <- ts.intersect(d.sub.inpc, d.sub.m.o, d.sub.remesas, d.sub.tasa.cambio)
datos.sub <- na.omit(datos.sub)
series <- colnames(datos.sub)
for( i in 1:dim(datos.sub)[2])
{
print( series[i])
print(adf.test(datos.sub[,i])$p.value)
acf(datos.sub[, i], main=series[i])
pacf(datos.sub[, i], main=series[i])
}
# Chunk 25
#################Graficamos
# el metodo facil
#############
par(mfrow = c(2, 2))
for(i in 1:4)
{
ts.plot(datos.sub[, i], col=colores[i],
main=paste0(as.character(colnames(datos.sub))[i], ' estacionalizado'),
xlab='',  ylab='',  lwd=2)
}
par(mfrow = c(1, 1))
# Chunk 26
####################
# Estimacion y suavecimiento
##################
muestra <- datos.sub
set.seed(0)
predic.VAR.p <- predict.VAR.init(ensamble=muestra)
M.predic.VAR <- mapply(predic.VAR.p, 1:200)
AI <- as.data.frame(t(M.predic.VAR))
AI$n <- 1:200
a <- ggplot(AI, aes(x=n ,y=d.sub.inpc, color=I(colores[4])))+
xlab('n obs. más nuevas usadas en el conjunto de entrenamiento')+
geom_line()+ theme_minimal() +ylab('SMAPE') +
ggtitle('Error en % al pronosticar el INP con datos mejor ubicados')
a
a <- ggplot(AI, aes(x=n ,y=d.sub.inpc, color=I(colores[4])))+
xlab('n obs. más nuevas usadas en el conjunto de entrenamiento')+
geom_line()+ theme_minimal() +ylab('SMAPE') +
ggtitle('Error en % al pronosticar el INP con datos mejor ubicados') +xlim(c(1,5))
a
# Chunk 27: modelo2
p <- VARselect(datos.sub, type = "const")$selection["AIC(n)"]
p
varc <- VAR(datos.sub, p = p)
summary(varc)
serial <- serial.test(varc)
arch <- arch.test(varc)
serial
arch
# Chunk 28
fore <- predict(varc, n.ahead = 1)
inpc[571-p]+fore$fcst$d.sub.inpc[1]
# Chunk 29
# causalidad de Granger
cau <- matrix(0, ncol(datos.sub), 1)
colnames(cau) <- "p.value"
rownames(cau) <- colnames(datos.sub)
for(i in 1 : ncol(datos.sub))
cau[i,] <- causality(varc, colnames(datos.sub)[i])$Granger$p.value # ya esta ortogonal
cau <- round(cau, 4)
cau
# Chunk 30: estimacionmodelo3
datos.sub.sin.remesas <- ts.intersect(d.sub.inpc, d.sub.m.o,
d.sub.tasa.cambio)
####################
# Estimacion y suavecimiento
##################
muestra <- datos.sub.sin.remesas
set.seed(0)
predic.VAR.p <- predict.VAR.init(ensamble=muestra)
M.predic.VAR <- mapply(predic.VAR.p, 1:200)
AI <- as.data.frame(t(M.predic.VAR))
AI$n <- 1:200
a <- ggplot(AI, aes(x=n ,y=d.sub.inpc, color=I(colores[5])))+
xlab('n obs. más nuevas usadas en el conjunto de entrenamiento')+
geom_line()+ theme_minimal() +ylab('SMAPE') +
ggtitle("Error en % al pronosticar el INP sin variable 'reservas' datos mejor ubicados")
a
# Chunk 31
p <- VARselect(datos.sub.sin.remesas,
type = "const")$selection["AIC(n)"]
p
varc <- VAR(datos.sub.sin.remesas, p = p)
#summary(varc)
serial <- serial.test(varc)
arch <- arch.test(varc)
serial
arch
# Chunk 32
p <- VARselect(datos.sub, type = "const")$selection["AIC(n)"]
p
varc <- VAR(datos.sub, p = p)
fore <- predict(varc, n.ahead = 1)
t1 <- Sys.time()
t2 <- Sys.time()
devtools::install_github("bmschmidt/wordVectors")
install.packages("devtools")
devtools::install_github("bmschmidt/wordVectors")
install.packages("Rtools")
###############################
# Scatterplot
###############################
datos <- iris
dim(iris)
plot(datos$Sepal.Length)
plot(datos$Sepal.Length, datos$Sepal.Width)
range(datos$Sepal.Length)
plot(datos$Sepal.Length, datos$Sepal.Width, xlim=c(4,6))
abline(v=5, col='red3', lwd=2) #vertical
abline(h=3, col='navy', lwd=3) #horizontal
abline(a=0, b=1/2, col='orange', lty='dashed', lwd=3)
######### Diferentes tamaños no jala el reciclyng
plot(datos$Sepal.Length, datos$Sepal.Width[1:149])
plot(datos$Sepal.Length, datos$Sepal.Width, col=colors()[1:150], cex=2, pch=20)
?plot
?cex
?phc
?pch
plot(datos$Sepal.Length, datos$Sepal.Width, col=colors()[1:150], cex=2, pch=20)
plot(datos$Sepal.Length, datos$Sepal.Width, col=colors()[1:50], cex=2, pch=1:150)
plot(datos$Sepal.Length, datos$Sepal.Width, col=colors()[1:50], cex=2, pch=1:25)
lines(seq(4.5, 8, by=.5), runif(8, 2, 4), col='red', lwd=6)
?pch  #mas facil de buscar en la doc
plot(datos$Sepal.Length, datos$Sepal.Width, col=colors()[1:50], cex=2, pch=1:25)
lines(seq(4.5, 8, by=.5), runif(8, 2, 4), col='red', lwd=6)
################################################
pairs(datos)
#Problema 1
#Inciso b
n<-100000
## SUELE FIJAR UNA SEMILLA CUANDO SIMULES, ES UNA BUENA PRACTICA
set.seed(123)
muestra<- rnorm(n,pi,sqrt(2))
ym<-cumsum(muestra)/(1:n)
plot(x=1:n,y=ym, type="l",xlab="Indice de muestra", ylab="Ym",main="Convergencia de la media experimental")
#Como la variable es normal, esta tiene media finita. Por tanto Ym converge a pi
#Inciso c
set.seed(0)
muestra<- rnorm(n,pi,sqrt(2))
ym<-cumsum(muestra)/(1:n)
plot(x=1:n,y=ym, type="l",xlab="Indice de muestra", ylab="Ym",main="Convergencia de la media experimental con 100 trayectorias")
# ponle tantito color
for(i in 2:100) {
muestra<- rnorm(n,pi,sqrt(2))
ym<-cumsum(muestra)/(1:n)
color <- runif(3,0,1)
lines(ym, col=rgb(color[1], color[2], color[3], 0.5))
}
#Inciso c
set.seed(0)
muestra<- rnorm(n,pi,sqrt(2))
ym<-cumsum(muestra)/(1:n)
plot(x=1:n,y=ym, type="l",xlab="Indice de muestra", ylab="Ym",main="Convergencia de la media experimental con 100 trayectorias")
# ponle tantito color
for(i in 2:100) {
muestra<- rnorm(n,pi,sqrt(2))
ym<-cumsum(muestra)/(1:n)
color <- runif(3,0,1)
lines(ym, col=rgb(color[1], color[2], color[3], 0.5))
}
plot(c(0,0))
plot(0)
x <- rnorm(100)
x <- c(x, NA)
mean(x)
s.na(x)
is.na(x)
mean(x[!is.na(x)])
plot(c(1,2), c(0,NA))
plot(c(1,NA), c(0,NA))
plot(c(NA,NA), c(0,NA))
plot(c(NA,1), c(0,NA))
######################################
# otro ejemplo de polimorfismo
######################################
cor(datos$Sepal.Length, datos$Sepal.Width)
datos
cor(datos)
sapply(datos, 2, class)
sapply(datos, class)
lapply(datos, class)
mapply(datos, class)
apply(datos, 2,class)
######################################
# ejercicio2 a
######################################
# librerias
library(wordVectors)
library(tsne)
library(tm)
library(quanteda)
library(dendextend)
#######################################
# funciones
M.td.spanol <- function(direccion)
{
#Obtencion de los conteos de palabras
# direccion (string): con la ruta donde se encuentran los archivos
corpus <- Corpus(DirSource(direccion, recursive=TRUE,
encoding = "UTF-8"),
readerControl=list(language="es"))
#preprocesamiento
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus,removeNumbers)
corpus <- tm_map(corpus,removePunctuation)
#corpus <- tm_map(corpus, content_transformer(tolower))
corp <- corpus(corpus)
tdm <- dfm(corp, tolower = TRUE, remove = stopwords('es'))
tdm <- dfm_tfidf(tdm)
return(tdm)
}
#######################################
path <- '~\\Desktop\\Third\\CD2\\Tarea3CD2Antonio\\'
setwd(path)
dir()
#                            iter=3,
#                            negative_samples=5)
# }
# model = train_word2vec("billions_sub.txt","billions_vectors.bin",
#                        vectors=200,
#                        threads=5,
#                        window=12,
#                        iter=3,
#                        negative_samples=5,force = T)
#############aqui empieza la accion
model <- read.vectors("billions_sub.bin")
dim(model)
set.seed(0)
#td <- M.td.spanol(direccion)
#saveRDS(td, file = "td_ejercicio2.rds")
td <- readRDS(file = "td_ejercicio2.rds")
apply(td, 1, summary)
dim(td)
gc()
#limite 100,000
busqueda <- round(dim(td)[2]/100) #tamaño del sampleo .1porciento
set.seed(0)
t1 <- Sys.time()
documento1  <- colnames(td[1,as.numeric(td[1,])>20])
documento2 <- colnames(td[2,as.numeric(td[2,])>20])
documento3 <- colnames(td[3,as.numeric(td[3,])>20])
ingredients <- unique(colnames(td))[sample(1:dim(td)[2], busqueda)]
dim(td) #478816 palabras
ingredients <- unique(colnames(td))[sample(1:dim(td)[2], busqueda)]
term_set <- lapply(ingredients,
function(ingredient)
{
nearest_words = model %>% closest_to(model[[ingredient]],10)
nearest_words$word
}) %>% unlist
subset <- model[[term_set,average=F]]
subset %>%
cosineDist(subset) %>%
as.dist %>%
hclust -> arbol
t1 <- t1 - Sys.time()
t1
colores <- c('#2894C0', rgb(71/255,230/255,127/255), 'purple')
dend <- as.dendrogram(arbol)
dend %>% color_labels ->dend
#plot(dend)
#labels_colors(dend) <- colorCodes[(term_set)][order.dendrogram(dend)]
dend <- color_branches(dend,k=3, col=colores)
plot(dend)
par(cex = 1)
dend %>% sort(type='nodes') %>% plot(main = "3-Clusters de una m.a. de términos(1%)")
clusters <- cutree(dend, k=3)
doc1.muestral <- names(clusters[clusters==1])
doc2.muestral <- names(clusters[clusters==2])
doc3.muestral <- names(clusters[clusters==3])
clusters <- cutree(dend, k=3)
doc1.muestral <- names(clusters[clusters==1])
doc2.muestral <- names(clusters[clusters==2])
doc3.muestral <- names(clusters[clusters==3])
documento1 <- colnames(td[1,as.numeric(td[1,])>0])
documento2 <- colnames(td[2,as.numeric(td[2,])>0])
documento3 <- colnames(td[3,as.numeric(td[3,])>0])
plot(model[[doc1.muestral,average=F]], method="pca")
plot(model[[doc1.muestral,average=F]], perplexity=200)
plot(model[[doc1.muestral,average=F]], perplexity=250)
doc1.muestral <- names(clusters[clusters==1])
doc2.muestral <- names(clusters[clusters==2])
doc3.muestral <- names(clusters[clusters==3])
documento1 <- colnames(td[1,as.numeric(td[1,])>0])
documento2 <- colnames(td[2,as.numeric(td[2,])>0])
documento3 <- colnames(td[3,as.numeric(td[3,])>0])
plot(model[[doc1.muestral,average=F]], perplexity=250)
sum(doc1.muestral %in% no.ceros.1)/sum(as.numeric(td[1,])>0)
sum(doc1.muestral %in% no.ceros.2)/sum(as.numeric(td[2,])>0)
sum(doc1.muestral %in% no.ceros.3)/sum(as.numeric(td[3,])>0)
sum(doc1.muestral %in% documento1)/sum(as.numeric(td[1,])>0)
sum(doc1.muestral %in% documento2)/sum(as.numeric(td[2,])>0)
sum(doc1.muestral %in% documento3)/sum(as.numeric(td[3,])>0)
###
plot(model[[doc2.muestral,average=F]], method="pca")
###
plot(model[[doc2.muestral,average=F]], perplexity=250)
sum(doc2.muestral %in% documento1)/sum(as.numeric(td[1,])>0)
sum(doc2.muestral %in% documento2)/sum(as.numeric(td[2,])>0)
sum(doc2.muestral %in%documento3)/sum(as.numeric(td[3,])>0)
#
plot(model[[doc3.muestral,average=F]], method="pca")
sum(doc3.muestral %in% documento1)/sum(as.numeric(td[1,])>0)
sum(doc3.muestral %in% documento2)/sum(as.numeric(td[2,])>0)
sum(doc3.muestral %in% documento3)/sum(as.numeric(td[3,])>0)
#
plot(model[[doc3.muestral,average=F]],perplexity=200)
par(cex=.5)
plot(model[[documento1, average=F]], perplexity=200)
par(cex=.7)
plot(model[[documento1, average=F]], perplexity=200)
plot(model[[documento1, average=F]], perplexity=150)
plot(model[[documento2, average=F]], perplexity=150)
par(cex=1)
plot(model[[documento3, average=F]], perplexity=100)
palabras.extranas <- as.data.frame(apply(td, 2, sum))
palabras.extranas$palabra <- row.names(palabras.extranas)
colnames(palabras.extranas) <- c( 'Rank', 'Palabra')
palabras.extranas <- palabras.extranas[ order(palabras.extranas$Rank),]
hist(palabras.extranas$Rank, xlim = c(0,100))
summary(palabras.extranas$Rank)
palabras <- head(palabras.extranas, 30)$Palabra
################
set.seed(0)
clustering <- kmeans(model, centers=model[[palabras,average=F]], iter.max = 100)
table(clustering$cluster)
doc1 <- names(clustering$cluster[clustering$cluster==6])
table(clustering$cluster)
doc1 <- names(clustering$cluster[clustering$cluster==5])
doc2 <- names(clustering$cluster[clustering$cluster==13])
doc3 <- names(clustering$cluster[clustering$cluster==21])
sum(doc1%in%no.ceros.1)/length(no.ceros.1)
sum(doc1%in%documento1)/length(documento1)
sum(doc1%in%documento2)/length(documento2)
sum(doc1%in%documento3)/length(documento3)
sum(doc2%in%documento1)/length(documento1)
sum(doc2%in%documento2)/length(documento2)
sum(doc2%in%documento3)/length(documento3)
sum(doc3%in%documento1)/length(documento1)
sum(doc3%in%documento2)/length(documento2)
sum(doc3%in%documento3)/length(documento3)
set.seed(0)
length(doc1.muestral
)
set.seed(0)
index <- sample(1:length(doc1.muestral),length(doc1.muestral)/10 )
plot(model[[doc1.muestral[index],average=F]], perplexity=250)
index <- sample(1:length(doc3.muestral),length(doc3.muestral)/10 )
plot(model[[doc3.muestral,average=F]],perplexity=200)
set.seed(0)
index <- sample(1:length(doc1.muestral),length(doc1.muestral)/100 )
plot(model[[doc1.muestral[index],average=F]], perplexity=250)
sum(doc1.muestral %in% documento1)/sum(as.numeric(td[1,])>0)
sum(doc1.muestral %in% documento2)/sum(as.numeric(td[2,])>0)
sum(doc1.muestral %in% documento3)/sum(as.numeric(td[3,])>0)
###
set.seed(0)
index <- sample(1:length(doc2.muestral),length(doc2.muestral)/100 )
plot(model[[doc2.muestral,average=F]], perplexity=250)
plot(model[[doc2.muestral,average=F]], perplexity=300)
plot(model[[doc2.muestral,average=F]], perplexity=100)
plot(model[[doc2.muestral[index],average=F]], perplexity=100)
plot(model[[doc3.muestral[index],average=F]],perplexity=200)
set.seed(0)
index <- sample(1:length(doc3.muestral),length(doc3.muestral)/100 )
plot(model[[documento1[index], average=F]], perplexity=150)
index <- sample(1:length(documento1),length(documento1)/100 )
plot(model[[documento1[index], average=F]], perplexity=150)
plot(model[[documento2[index], average=F]], perplexity=150)
plot(model[[documento1[index], average=F]], perplexity=150)
plot(model[[documento2[index], average=F]], perplexity=150)
plot(model[[documento3[indes], average=F]], perplexity=100)
plot(model[[documento3[index], average=F]], perplexity=100)
index <- sample(1:length(documento2),length(documento2)/100 )
plot(model[[documento2[index], average=F]], perplexity=150)
