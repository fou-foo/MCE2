apriori <- apriori[ order(apriori$grams.2, decreasing = TRUE), ]
apriori$grams.2 <- as.character(apriori$grams.2)
apriori$from <- mapply(apriori$grams.2, FUN=
function(x) strsplit(x, split='_')[[1]][1] )
apriori$to <- mapply(apriori$grams.2, FUN=
function(x) strsplit(x, '_')[[1]][2] )
Estados <- unique(train$Tag)
(Estados <- sort(Estados))
n <- length(Estados)
Markov <- matrix(rep(0, n**2), ncol = n )
colnames(Markov) <- row.names(Markov) <- Estados
row.names(apriori) <- apriori$grams.2
# obtenemos las frecuencias de los tags singletones
palabras <- as.data.frame(table(train$Tag))
colnames(palabras) <- c('Tag', 'Freq')
palabras <- palabras[ order(palabras$Tag), ]
row.names(palabras) <- palabras$Tag
for (i in Estados)
{
for(j in  Estados)
{
par.tj.ti <- apriori[paste0(i, '_' ,j), 'Freq']
salida.i <-  palabras[as.character(i), 'Freq']
Markov[as.character(i), as.character(j)] <- par.tj.ti/salida.i
}
}
index <- is.na(Markov)
sum(index)
Markov[index] <- 0
#verificamos
a <- apply(Markov, 1, sum)
a
image(Markov)
########### vamos por las verosimilitudes
library(dplyr) #agrupamos para contar los pares (word,tag)
vero <- train
vero$pares <- paste0(vero$Palabra, '_', vero$Tag)
vero <- vero %>% group_by(Palabra, Tag, pares) %>% summarise(Freq=n())
vero <- vero[order(vero$Tag, decreasing = FALSE),]
index <- grep("^$", vero$Palabra)
vero <- vero[-index, ]
p <- length(unique(vero$Palabra))
Verosimil <- matrix(rep(0, n*p), nrow = n, ncol = p)
row.names(Verosimil) <- Estados
colnames(Verosimil) <- sort(unique(vero$Palabra))
dim(Verosimil)
row.names(vero) <- vero$pares
for(tag in row.names(Verosimil))
{
for (word in colnames(Verosimil))
{
palabra.tag <- as.numeric(vero[paste0( word, '_', tag), 'Freq'])
no.tag <- palabras[as.character(tag), 'Freq']
Verosimil[as.character(tag), as.character(word)] <- palabra.tag/ no.tag
}
}
index <- is.na(Verosimil)
sum(index)
Verosimil[index] <- 0
#verificacion
a <- apply(Verosimil, 1, sum)
#names(a) <- NULL
a
image(Verosimil)
####################################################
#Tokenizacion
# ejercicio b)
###################################################
texto.prueba <- "Your contribution to Goodwill will mean more than you may know ."
texto <- strsplit(texto.prueba, split=' ')
texto <- unlist(texto)
index <- match(texto, colnames(Verosimil))
out <- Verosimil[,index ]
out <- as.data.frame(out)
salida <- list()
for (i in colnames(out))
{
index <- which(out[,as.character(i)] >0 )
elejidos <-  row.names(out)[index]
salida <- c(salida, list(palabra.tag=c(i,elejidos )))
}
salida
###############################
# ejercicio d)
################################
library(HMM)
dim(Verosimil)
path <- "C:/Users/fou-f/Desktop/Third/CD2/Tarea2/POSData/"
dim(Markov)
###############################
# ejercicio d)
################################
library(HMM)
dim(Verosimil)
dim(Markov)
set.seed(0)
p <- rep(1/n,n)
sum(p)
hmm <- initHMM(States = as.character(palabras$Tag),
sort(unique(train$Palabra)),
startProbs = p,
transProbs = Markov,
emissionProbs = (Verosimil))
sort(unique(train$Palabra))
dim(Verosimil)
dim(Markov)
row.names(Verosimil)
row.names(Markov)
row.names(Verosimil)
colnames(Verosimil)
hmm <- initHMM(States = row.names(Verosimil),
Symbols = colnames(Verosimil),
startProbs = p,
transProbs = Markov,
emissionProbs = (Verosimil))
texto <- c("Your contribution to Goodwill will mean more than you may know",".")
palabras <- unlist(strsplit(tolower(texto)," "))
palabras
viterbi(hmm, palabras)
?viterbi
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
texto <- 'Coming to Goodwill was the first step toward my becoming totally'
texto <- 'Coming to Goodwill was the first step toward my becoming totally'
palabras <- unlist(strsplit(tolower(texto)," "))
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally'
palabras <- unlist(strsplit(tolower(texto)," "))
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
library(xtable)
xtable(salida)
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally'
palabras <- unlist(strsplit(tolower(texto)," "))
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
Tag=viterbi(hmm, palabras)
palabras %in% unique(train$Palabra)
palabras
View(train)
View(apriori)
View(vero)
train %>%
library(xtable)
names(train)
train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
a
View(a)
palabras[11]
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
hmm <- initHMM(States = row.names(Verosimil),
Symbols = colnames(Verosimil),
startProbs = p,
transProbs = Markov,
emissionProbs = (Verosimil))
texto <- c("Your contribution to Goodwill will mean more than you may know",".")
palabras <- unlist(strsplit(tolower(texto)," "))
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
palabras
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
#
library(coreNLP)
hmm <- initHMM(States = row.names(Verosimil),
Symbols = colnames(Verosimil),
startProbs = p,
transProbs = Markov,
emissionProbs = (Verosimil))
texto <- c("Your contribution to Goodwill will mean more than you may know",".")
palabras <- unlist(strsplit(tolower(texto)," "))
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally.'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
############suavizamiento
#
#######################
path <- "C:/Users/fou-f/Desktop/Third/CD2/Tarea2/POSData/"
dir(path)
#pre procesamiento al archivo
row <- readLines(paste0(path, 'training.pos'))
str(row)
length(row)
y <- lapply( FUN = function(x)  {
a <- unlist(strsplit(row[x], split='\t'))[1]
return(a)
},X= 1:length(row))
y <- unlist(y)
z <- lapply( FUN = function(x){
a <- unlist(strsplit(row[x], split='\t'))[2]
return(a)
},X= 1:length(row))
z <- unlist(z)
train <- data.frame(palabra=as.character(y), tag=as.character(z))
names(train) <- c('Palabra', 'Tag')
train$Palabra <- as.character(train$Palabra)
train$Tag <- as.character(train$Tag)
unique(train$Tag) # todos los posibles tags
texto <- paste0(train$Tag, collapse = ' ')
library(quanteda)
#obtenemos las frecuencias de los bigramas y ordenamos lexicografimente
grams.2 <- tokens(texto) %>%
tokens_ngrams(n=2)
grams.2 <- grams.2$text1
apriori <- table(grams.2)
apriori <- as.data.frame(apriori)
apriori <- apriori[ order(apriori$grams.2, decreasing = TRUE), ]
apriori$grams.2 <- as.character(apriori$grams.2)
apriori$from <- mapply(apriori$grams.2, FUN=
function(x) strsplit(x, split='_')[[1]][1] )
apriori$to <- mapply(apriori$grams.2, FUN=
function(x) strsplit(x, '_')[[1]][2] )
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
palabras
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit((texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
path <- "C:/Users/fou-f/Desktop/Third/CD2/Tarea2/POSData/"
dir(path)
#pre procesamiento al archivo
row <- readLines(paste0(path, 'training.pos'))
str(row)
length(row)
y <- lapply( FUN = function(x)  {
a <- unlist(strsplit(row[x], split='\t'))[1]
return(a)
},X= 1:length(row))
y <- unlist(y)
z <- lapply( FUN = function(x){
a <- unlist(strsplit(row[x], split='\t'))[2]
return(a)
},X= 1:length(row))
z <- unlist(z)
train <- data.frame(palabra=as.character(y), tag=as.character(z))
names(train) <- c('Palabra', 'Tag')
train$Palabra <- as.character(train$Palabra)
train$Tag <- as.character(train$Tag)
unique(train$Tag) # todos los posibles tags
texto <- paste0(train$Tag, collapse = ' ')
library(quanteda)
#obtenemos las frecuencias de los bigramas y ordenamos lexicografimente
grams.2 <- tokens(texto) %>%
tokens_ngrams(n=2)
grams.2 <- grams.2$text1
apriori <- table(grams.2)
apriori <- as.data.frame(apriori)
apriori <- apriori[ order(apriori$grams.2, decreasing = TRUE), ]
apriori$grams.2 <- as.character(apriori$grams.2)
apriori$from <- mapply(apriori$grams.2, FUN=
function(x) strsplit(x, split='_')[[1]][1] )
apriori$to <- mapply(apriori$grams.2, FUN=
function(x) strsplit(x, '_')[[1]][2] )
Estados <- unique(train$Tag)
(Estados <- sort(Estados))
n <- length(Estados)
Markov <- matrix(rep(0, n**2), ncol = n )
colnames(Markov) <- row.names(Markov) <- Estados
row.names(apriori) <- apriori$grams.2
# obtenemos las frecuencias de los tags singletones
palabras <- as.data.frame(table(train$Tag))
colnames(palabras) <- c('Tag', 'Freq')
palabras <- palabras[ order(palabras$Tag), ]
row.names(palabras) <- palabras$Tag
for (i in Estados)
{
for(j in  Estados)
{
par.tj.ti <- apriori[paste0(i, '_' ,j), 'Freq']
salida.i <-  palabras[as.character(i), 'Freq']
Markov[as.character(i), as.character(j)] <- par.tj.ti/salida.i
}
}
index <- is.na(Markov)
sum(index)
Markov[index] <- 0
#verificamos
a <- apply(Markov, 1, sum)
a
image(Markov)
########### vamos por las verosimilitudes
library(dplyr) #agrupamos para contar los pares (word,tag)
vero <- train
vero$pares <- paste0(vero$Palabra, '_', vero$Tag)
vero <- vero %>% group_by(Palabra, Tag, pares) %>% summarise(Freq=n())
vero <- vero[order(vero$Tag, decreasing = FALSE),]
index <- grep("^$", vero$Palabra)
vero <- vero[-index, ]
p <- length(unique(vero$Palabra))
Verosimil <- matrix(rep(0, n*p), nrow = n, ncol = p)
row.names(Verosimil) <- Estados
colnames(Verosimil) <- sort(unique(vero$Palabra))
dim(Verosimil)
row.names(vero) <- vero$pares
for(tag in row.names(Verosimil))
{
for (word in colnames(Verosimil))
{
palabra.tag <- as.numeric(vero[paste0( word, '_', tag), 'Freq'])
no.tag <- palabras[as.character(tag), 'Freq']
Verosimil[as.character(tag), as.character(word)] <- palabra.tag/ no.tag
}
}
index <- is.na(Verosimil)
sum(index)
Verosimil[index] <- 0
#verificacion
a <- apply(Verosimil, 1, sum)
#names(a) <- NULL
a
image(Verosimil)
####################################################
#Tokenizacion
# ejercicio b)
###################################################
texto.prueba <- "Your contribution to Goodwill will mean more than you may know ."
texto <- strsplit(texto.prueba, split=' ')
texto <- unlist(texto)
index <- match(texto, colnames(Verosimil))
out <- Verosimil[,index ]
out <- as.data.frame(out)
salida <- list()
for (i in colnames(out))
{
index <- which(out[,as.character(i)] >0 )
elejidos <-  row.names(out)[index]
salida <- c(salida, list(palabra.tag=c(i,elejidos )))
}
salida
###############################
# ejercicio d)
################################
library(HMM)
dim(Verosimil)
dim(Markov)
row.names(Verosimil)
row.names(Markov)
set.seed(0)
p <- rep(1/n,n)
sum(p)
hmm <- initHMM(States = row.names(Verosimil),
Symbols = colnames(Verosimil),
startProbs = p,
transProbs = Markov,
emissionProbs = (Verosimil))
texto <- c("Your contribution to Goodwill will mean more than you may know",".")
palabras <- unlist(strsplit(tolower(texto)," "))
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
texto <- c("Your contribution to Goodwill will mean more than you may know",".")
palabras <- unlist(strsplit(texto," "))
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
texto <- c("Your contribution to Goodwill will mean more than you may know",".")
palabras <- unlist(strsplit(tolower(texto)," "))
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
library(xtable)
xtable(salida)
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
salida
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit((texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'dig'
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
salida
View(vero)
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[10] <- 'total'
palabras %in% unique(train$Palabra)
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[10] <- 'total'
palabras
##########ejercicio e
#
###################
texto <- 'Coming to Goodwill was the first step toward my becoming totally .'
palabras <- unlist(strsplit(tolower(texto)," "))
palabras %in% unique(train$Palabra)
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'total'
palabras
#checar palabra poco comun
a <- train %>% select(Palabra)%>% group_by(Palabra) %>% summarise(n = n())
a <- a[order(a$n), ]
palabras[11] <- 'total'
palabras %in% unique(train$Palabra)
salida <- data.frame(palabras=palabras, Tag=viterbi(hmm, palabras))
salida
library(xtable)
xtable(salida)
salida
