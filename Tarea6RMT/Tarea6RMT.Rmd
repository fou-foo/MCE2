---
title: "Temas selectos de econometría y finanzas (modulo de matrices aleatorias)"
author: "J. Antonio García Ramirez, Tarea 6: Aplicaciones a datos financieros en el contexto de big data."
date: "14 de noviembre, 2018"
output: pdf_document
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE, cache=TRUE)
library(BatchGetSymbols) # obtencion de series
library(reshape2)
library(lubridate)     #manejo de fechas
library(dplyr)
library(tseries)
library(parallel)
library(readr) #lectura rapida
library(seasonal)
library(RMTstat)
library(pls)
```

```{r funciones.auxiliares, echo=FALSE}
SMAPE <- function(y.hat, y)
{
  # Calculo de raiz cuadrada de error cuadratico medio
  return( sum( abs(y.hat-y) /(abs(y) + abs(y.hat))   )*(100/length(y)) )
}
ERROR <- function(y.hat, y)
{
  # Calculo de raiz cuadrada de error cuadratico medio
  return( mean( abs(y.hat-y) )/length(y)) 
}
quita.tendencia.init <- function(data, inicio , frecuencia)
{
  # CLOSURE para quitar seasonality, regresa una funcion
  #  data (vector): valores de la serie de tiempo 
  # inicio (vector.longitud2): inicio de la serie de tiempo c(2008,1)
  # frecuencia (numeric): frecuencia de la serie (semanal:54)
  inicio <- inicio 
  frecuencia <- frecuencia
  function(data)
  {
    s <- ts(data, start = inicio, frequency = frecuencia) # habra que harcodear estos numeros
    x <- tryCatch(seas(s)$series$s11,
                         error = function(e) data, finally = data )
    x <- as.numeric(x)
    return(x)
  }
}
adf.test.custom <- function(y, option='both')
{
  # funcion para elegir el resago optimo
  # y (numeric): vector con los datos de la serie de tiempo univariada
  # option (chacaracter) : eleccion de la tendencia e intercepto 'none','c','t','both'
  y <- ts(y) 
  lag <- floor(log(length(y))) + 1 #acotamos el numero de lags por el que siguiere el 
  #texto de Chan Ngai
  datos <- data.frame(y1 = diff(y))
  for (i in 2:lag) #aumentamos las columnas de lags 
  {
    datos[, as.character(paste0('y',i))] <- c(diff(y, lag=i), rep(NA, i-1))
  }
  names(datos) <- c('y1', names(datos)[2:lag])
  if (option == 'none')
  {
    #aplicamos el test para cada lag
    resultado <- mapply(function(x)
    {
      formula <- paste(names(datos)[x], collapse = '+')
      formula <- as.formula(paste0('y1 ~ ', formula, '-1'))
      modelo <- lm(formula , data = datos )
      resumen <- summary(modelo)
      # nos fijamos si todos los coeficientes de la regresion
      # son significativos individualmente
      coeficientes.significativos <- resumen$coefficients[, 'Pr(>|t|)']
      coeficientes.significativos <- coeficientes.significativos <= 0.05
      if(sum(coeficientes.significativos) == 1)
      {
        big <- BIC(modelo)
        # en caso de que todos los coeficientes sean significativos regresamos 
        # el BIC de la regresion
        return(big)
      } else {return(Inf)} #si un coeficiente al menos es no significativo
      #regresamos un BIC infinito
    }, 2:lag)
  }
  
  if (option == 'c')
  {
    datos[, 'c'] <- rep(1, dim(datos)[1] )
    #aplicamos el test para cada lag
    resultado <- mapply(function(x)
    {
      formula <- paste(names(datos)[x], collapse = '+')
      formula <- as.formula(paste0('y1 ~ ', formula))
      modelo <- lm(formula , data = datos )
      resumen <- summary(modelo)
      # nos fijamos si todos los coeficientes de la regresion
      # son significativos individualmente
      coeficientes.significativos <- resumen$coefficients[, 'Pr(>|t|)']
      coeficientes.significativos <- coeficientes.significativos <= 0.05
      if(sum(coeficientes.significativos) == 2)
      {
        big <- BIC(modelo)
        # en caso de que todos los coeficientes sean significativos regresamos 
        #el BIC de la regresion
        return(big)
      }else {return(Inf)} #si un coeficiente al menos es no significativo 
      #regresamos un BIC infinito
    }, 2:lag)
  }
  if (option == 't')
  {
    datos[, 't'] <- cumsum(1:dim(datos)[1])
    #aplicamos el test para cada lag
    resultado <- mapply(function(x)
    {
      formula <- paste(c(names(datos)[x], 't'), collapse = '+')
      formula <- as.formula(paste0('y1 ~ ', formula, '-1'))
      modelo <- lm(formula , data = datos )
      resumen <- summary(modelo)
      # nos fijamos si todos los coeficientes de la regresion
      # son significativos individualmente
      coeficientes.significativos <- resumen$coefficients[, 'Pr(>|t|)']
      coeficientes.significativos <- coeficientes.significativos <= 0.05
      if(sum(coeficientes.significativos) == 2)
      {
        big <- BIC(modelo)
        # en caso de que todos los coeficientes sean significativos regresamos 
        #el BIC de la regresion
        return(big)
      }else { return(Inf)}  #si un coeficiente al menos es no significativo
      #regresamos un BIC infinito
    }, 2:lag)
  }
  if (option == 'both')
  {
    datos[, 't'] <- cumsum(1:dim(datos)[1])
    #aplicamos el test para cada lag
    resultado <- mapply(function(x)
    {
      formula <- paste(c(names(datos)[2:(x)], 't'), collapse = '+')
      formula <- as.formula(paste0('y1 ~ ', formula))
      modelo <- lm(formula , data = datos )
      resumen <- summary(modelo)
      # nos fijamos si todos los coeficientes de la regresion
      # son significativos individualmente
      coeficientes.significativos <- resumen$coefficients[, 'Pr(>|t|)']
      coeficientes.significativos <- coeficientes.significativos <= 0.05
      if(sum(coeficientes.significativos) == 3)
      {
        big <- BIC(modelo)
        # en caso de que todos los coeficientes sean significativos regresamos
        #el BIC de la regresion
        return(big)
      } else { return(Inf)}  #si un coeficiente al menos es no significativo
      #regresamos un BIC infinito
    }, 2:lag)
  }
  parsimonia <- which.min(resultado) 
  names(parsimonia) <- 'Lag optimo'
  return(parsimonia)
}
```



# Ejercicio 1

En este ejercicio se busca integrar los conocimientos aprendidos a lo largo del curso. Para  ello se solicita realizar lo siguiente:

a.  *Completar la derivación de la distribución de Marcenko-Pastur, partiendo de las notas de clase. Sea los más claro posible, sin omitir ningún detalle algebraico (puede escanearlo.)*

b.  *Reproduzca la figura 14.1 del libro seguido en este módulo (__Introduction to Random Matrices__, G. Livan et. al.), bajo las mismas condiciones y parámetros (compruebe que $p > 0.05$ en el test de Kolmogorov-Smirnov).*

```{r b, echo=FALSE}

f.densidad <-function(N,M,beta)
{ 
  # Entradas:
      # N (numeric): numero de variables 
      # M (numeric): numero de observaciones
      # beta(numeric): indica el ensamble LOE (1), LSE(4) y (2) LUO
  if(beta == 1)
  {
    #Caso LOE
    c <- N/M # la razon entre las dimensiones de la matriz
    H <- matrix(rnorm(n = N*M,mean = 0,sd = 1),nrow = N,ncol = M ) #construimos la matriz densa
    H <- H%*%t(H) # la hacemos simetrica
    VP <- eigen(H)$values
    }
  if(beta == 2)
  {
    #CASO LUE
    c <- N/M
    Real <- rnorm(N*M,mean = 0,sd = 1)
    Imaginaria <- rnorm(N*M,mean = 0,sd = 1)
    H<- matrix(complex(real = Real,imaginary = Imaginaria),nrow = N,ncol = M )
    H_t<- t(matrix(complex(real = Real,imaginary = -Imaginaria),nrow = N,ncol = M ))
    W <- H%*%(H_t)
    VP <- eigen(W)$values
  }
  if(beta == 4)
  {
    # caso LSE
    library(QZ)
    Real <- rnorm(N*M,mean = 0,sd = 1)
    Imaginaria <- rnorm(N*M,mean = 0,sd = 1)
    A <- matrix(complex(real = Real,imaginary = Imaginaria),nrow = N,ncol = M )
    A_c <- matrix(complex(real = Real,imaginary = -Imaginaria),nrow = N,ncol = M )
    
    Real <- rnorm(N*M,mean = 0,sd = 1)
    Imaginaria <- rnorm(N*M,mean = 0,sd = 1)
    B <- matrix(complex(real = Real,imaginary = Imaginaria),nrow = N,ncol = M )
    B_c <- matrix(complex(real = Real,imaginary = -Imaginaria),nrow = N,ncol = M )
    
    aux1 <- cbind(A,B)
    aux2 <- cbind(-B_c,A_c)
    H <- rbind(aux1,aux2)
    W <- H%*%H(H)
    VP <- eigen(W)$values
  }
  return(VP)
}
set.seed(0)
#######Esta parte dibuja el histograma con la N,M y beta de parametros (c = N/M con N<M)
#N <- 100
#M <- 800
#beta <- 4
#m <- f.densidad(N, M, beta)
#L <- array(0,dim = c(length(M),0))
#for (i in 1:5000) 
#{
 # m2 <- f.densidad(N,M,beta)
#  L <- cbind(L, m2)
  #print(i)
#}
#hist(L,breaks = 100,border = TRUE)
#library(ggplot2)
#qplot(as.vector(L),geom = "freqpoly",breaks = seq(min(L),max(L),50))
###### Hasta aqu?

##### Ahora el grafico con puntos y uso la funcion p(VP,beta,N)
p <- function(x,beta,N)
{
  # x (numeric): soporte
  # N (numeric): 
  1/(beta*N)*p_MP(x/(beta*N))
}
p_MP <- function(y)
{
  # soporte de la distribucion 
  zeta_menos <- (1-c^(-1/2))^2
  zeta_mas <- (1+c^(-1/2))^2
  a <- 1 / (2*pi*y) * sqrt( ( y-zeta_menos)*(zeta_mas - y))
  return(a)
}
set.seed(0)
VP <- f.densidad(N=100,M=200,beta=1)
c <- 100/200
plot(VP/100, p(VP,beta = 1,N = 100)*100, pch= 20,col="hotpink",xlim = c(-1,16),ylab = "p(x)", ylim=c(0,.5))
VP <- f.densidad(N=100,M=800,beta=1)
c <- 100/800
points(VP/100, p(VP,beta = 1,N = 100)*100, pch= 20,col="palegreen4")
VP <- f.densidad(N=100,M=200,beta=2)
c <- 100/200
points(VP/200, p(VP,beta = 2,N = 100)*200, pch= 18,col="paleturquoise")
VP <- f.densidad(N=100,M=800,beta=2)
c <- 100/800
points(VP/200,p(VP,beta = 2,N = 100)*200,pch= 18,col="yellow3")
VP <- f.densidad(N=100,M=200,beta=4)
c <- 100/200
x <- 0
points(c(0, VP/400, 7), p(c(0, VP, 7),beta = 4,N = 100)*400, pch= 4,col="blue", type='l')
VP <- f.densidad(N=100,M=800,beta=4)
c <- 100/800
points(VP/400, p(VP,beta = 4,N = 100)*400, pch= 4,col="red", type='l')
legend("topright", inset=0,
       c("b = 1"," b = 1","b= 2"," b =2","c=1/2 b =4","c=1/8 b=4"),
       col=c("hotpink","palegreen4","paleturquoise","yellow3","blue","red"),
       horiz=F, cex=.6,pch = c(19,19,19,19,4,4),xjust = 0,yjust = 0)
```

c.  *Descargue las series de tiempo que componen el índice bursátil Standard & Poor’s 500. Utilizando una periodicidad semanal durante los últimos 10 años (Enero 2008 a la fecha).*

Descargamos los datos en tiempo real y gráficamos algunas de las series. Utilizamos 
los tickers de las empresas que contribuyen al S&P500 previamente de [https://mx.investing.com/indices/investing.com-us-500-components](https://mx.investing.com/indices/investing.com-us-500-components).

```{r parametros, echo=FALSE}
diferencia <- today()- ymd('2008-01-01')
fecha <- as.numeric(diferencia)
today()-days(fecha) #checar fecha de inicio
first.date <- Sys.Date() - fecha #actualización en tiempo real 
last.date <- Sys.Date() 
freq.data <- 'weekly'  # frecuencia semanal 
```

```{r descarga.de.datos, echo=FALSE}
# lectura de tickerts
Componentes_Investing_com_United_States_500 <- read_csv("Componentes Investing.com United States 500.csv") # previanmente descargamos de
       #https://mx.investing.com/indices/investing.com-us-500-components los nombres de las empresas
tickers <- Componentes_Investing_com_United_States_500$Símbolo
companias <- BatchGetSymbols(tickers = tickers, 
                         first.date = first.date,
                         last.date = last.date, 
                         freq.data = freq.data,
                         do.complete.data = FALSE) #sihay nulos los descartamos
# comprobamos que variable es la que se registra 'price.close '
#a <- companias$df.tickers
#a <- subset(a, ticker=='A')
#sapply(a,class )
#a <- a[ a$price.open !=a$price.high,  ]
#a <- a[ a$price.low !=a$price.high,  ]
#a <- a[ a$price.low !=a$price.close ,  ]
#a <- a[ a$price.adjusted !=a$price.close ,  ]
#a <- unique(as.data.frame(a)) #identificamos la variable de interes
serie <- companias$df.tickers
class(serie) <- 'data.frame'
serie %>% select(ticker, ref.date, price.close ) -> serie#  era open o close ?
```


d.  *Aplique las transformaciones necesarias (aprendidas en el módulo de series de tiempo) para trabajar las series de tiempo desde el punto de vista estacionario. Deseche las series de los mercados que presentan problemas.*

Se procedió a aplicar la transformación logaritmo para disminuir la varianza de las series originales, también se aplicaron técnicas para eliminar la estacionalidad en las pocas series que la presentan. El resultado es un conjunto de datos (de dimensiones $565,449$) donde las observaciones son semanas registradas y las columnas las empresas que reportan su indicador de cierre. También se determino un resago de 8 para todas las series para estacionalizar las series, la prueba de Anderson Darling descarta que tengamos raices unitarias. 

```{r limpieza, echo=FALSE }
SP500 <- BatchGetSymbols(tickers = "^GSPC", 
                         first.date = first.date,
                         last.date = last.date, 
                         freq.data = freq.data,
                         do.complete.data = FALSE, #sihay nulos los descartamos
                         cache.folder = file.path(tempdir(),'BGS_Cache')) 
#names(SP500)
SP500 <- as.data.frame(SP500$df.tickers)[, c('ref.date', 'price.open')]
sp.500 <- na.omit(SP500)
serie2 <- dcast(serie, ref.date ~ ticker, value.var = 'price.close' )
#write_csv(serie2, path='serie2.csv')
#serie2 <- read_csv( file='serie2.csv')
serie3 <- apply(serie2, 2, function(x) sum(is.na(x))) # identificamos series problematicas
#table(serie3)
malas <- which(serie3 > 8 )
serie4 <- serie2[,  !(colnames(serie2) %in% names(malas)) ]
serie5 <- na.omit(serie4)
class(serie5) <- 'data.frame'
serie5$ref.date <- as.Date(serie5$ref.date)
serie.cruda <- serie5 # para comparacion sin estacionalizar
#par(mfrow=c(3,3))
#inspeccion visual 
#for(i in 1:(dim(serie5)[2]-1))
#{
 # s <- ts(serie5[, i], start = c(2008,1), frequency = 54)
#  class(s)
#  if(i %% 20==0) plot(s, main=as.character(names(serie5)[i]), xlab='series brutas')
#}
#par(mfrow=c(1,1))
# quitamos tendencia 
quita.tendencia <- quita.tendencia.init(inicio= c(2008, 1),frecuencia = 12 )
series <- mclapply(FUN=quita.tendencia, serie5[,2:dim(serie5)[2]], mc.cores = 6)
series <- as.data.frame(series)
```

```{r detalle, echo = FALSE}
series$time <- serie5$ref.date
series.ex <- log(series[, -dim(series)[2]]) # aplicamos logaritmo para estabilizar la varianza
series.ex$time <- series$time
#par(mfrow=c(3,3))
#inspeccion visual sin seasonality
#for(i in 1:(dim(series)[2]-1))
#{
 # s <- ts(series.ex[, i], start = c(2008,1), frequency = 54)
#  if(i %% 20==0) plot(s, main=as.character(names(series.ex)[i]), xlab='log(serie)')
#}
#par(mfrow=c(1,1))
# resagos
resagos <- matrix(rep(0, (dim(series.ex)[2]-1)*2), ncol=2)
#dim(resagos)
colnames(resagos) <- c('adf', 'adf.propio')
for(i in 1:(dim(series.ex)[2]-1))
{
  resagos[i, 'adf'] <- adf.test(ts(series.ex[,i]))$parameter
  resagos[i, 'adf.propio'] <- adf.test.custom(ts(series.ex[,i]))
}
#resagos
#apply(resagos, 2, mean)
resago <- 8
# diferenciar
serie.chida <- apply(series.ex[,-dim(series.ex)[2]], 2, function(x) diff(x, lag=resago))
serie.chida <- as.data.frame(serie.chida)
serie.chida$time <- serie5$ref.date[-(1:resago)]
# test de raices
p.value <- rep(5, dim(serie.chida)[2]-1)
for(i in 1:(dim(serie.chida)[2]-1))
{
  p.value[i] <- adf.test(serie.chida[,i ])$p.value
}
#hist(p.value) # ya son estacionales
#p.value # porque son menores a 0.05
# visualizar
par(mfrow=c(3,3))
#inspeccion visual sin tendencia
#for(i in 1:(dim(serie.chida)[2]-1))
#{
 # s <- ts(serie.chida[, i], start = c(2008,2), frequency = 54) #manual
  #if(i %% 20==0) plot(s, main=as.character(names(serie.chida)[i]), xlab='resago:8')
  #if(i %% 9==0) a <- scan()
#}
par(mfrow=c(1, 1))
# visualizacion conjunta
library(ggplot2)
a <- melt(serie.chida, id='time' )
#names(a)
ggplot(a, aes(x=time, y=value)) + geom_line(aes(colour=variable), alpha=0.2) +
  guides(colour=FALSE)+ theme_minimal()+ggtitle('Series estacionales y ergodicas')
#sp.500 <- ts(sp.500$price.close, start=2008, fre=54)
#plot.ts(sp.500, main='SP500', col='purple')
```




e.  *Determine el número de componentes significativos adecuando un test derivado de la distribución de Marcenko-Pastur.*


En vista de que conocemos la distribución asintótica de Marchenko Pastur, el criterio que determinamos se construye __contemplando únicamente los valores propios que son mayores a la esperanza de la distribución__, para ello la estimamos con una muestra 10 veces más grande que el número de registros de nuestro conjunto de datos. El criterio dice que usemos 60 componentes

```{r mp1, echo=FALSE}
##################################
# interseccion de series
#################################
names(SP500) <- c('time', 'price.close.SP')
m <- merge(serie.chida, SP500, by.x='time', by.y='time')
colnames(m) <- c(colnames(m)[1:(dim(m)[2]-1)], 'y')
tiempo <- m$time 
#tiempo
index <- which(tiempo>ymd('2018-01-01'))
m$time <- NULL
train <- m[-index, ]
test <- m[index,]
vals <- eigen(cor(scale(train[, -dim(train)[2]])))$values
# minisimulacion
set.seed(0)
#names(train)
r <- (dim(train)[2]-1)/dim(train)[1]
x <- c(0,seq((1-r**.5)**2, (1+r**.5)**2, by=0.1)) #soporte de la distribucion M-P
plot(x,dmp(x, ndf=dim(train)[1], pdim=dim(train)[2]-1 ), col='purple', type='l', 
     ylab='', main='Distribución limite Marchenko-Pastur')
rug(vals, col='red')
muestras <- dim(m)[1]*10
set.seed(0)
limite <- mean(rmp(muestras, ndf=dim(train)[1], pdim=dim(train)[2]-1 ))
abline(v=limite)
vals <- vals[vals>limite]
print('Numero de componentes segun M-K')
(RMT.cota <- length(vals)) #60 cota criterio de M-P

```



f.  *Aplique regresión por componentes principales utilizando el número de componentes sugeridos por el resultado de matrices aleatorias y compare el resultado utilizando el criterio del 80% de la varianza. Se busca predecir el valor de apertura del índice S&P500 el lunes por la mañana a traves de los 500 mercados que lo componen.*

Para poder medir el error en porcentaje utilizamos el [SMAPE](https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error) cuyo rango es [0,100] donde 0 es un error de 0\% (predicción perfecta) y 100 una predicción totalmente erronea. También utilizamos el error promedio sobre las predicciones del año 2008. Los resultados son de 72% aprox. en ambos casos.

```{r pcr, echo =FALSE}
################# resultado con RMT
modelo.pcr.rmt <- pcr(y~., data=train, ncomp=RMT.cota)
#summary(modelo.pcr.rmt)
y.hat.test <- predict(modelo.pcr.rmt, ncomp=RMT.cota , newdata = test)
print('Presición usando criterio M-k')
(100-SMAPE(test$y, as.numeric(y.hat.test))) #Presicion SMAPE
(100-ERROR(test$y, as.numeric(y.hat.test))) #Presicion promedio
res1 <- data.frame(y=test$y, y.hat=as.numeric(y.hat.test),
                   time=tiempo[-(1:(dim(train)[1])) ])
p1 <- ggplot(res1, aes(x=time, y=y))+geom_line(color=I('purple')) + theme_minimal()+
  geom_line(data=res1, aes(x=time, y=y.hat),color=I('orange'))+
  ggtitle('Pronostico para 2018,SP800, criterio MP y PCR')
################# resultado con 80 %vars
modelo.pcr.rmt <- pcr(y~., data=train, ncomp=24)
y.hat.test <- predict(modelo.pcr.rmt, ncomp=24, newdata = test)
print('Presición usando criterio PCR 80% de varianza')
(100-SMAPE(test$y, as.numeric(y.hat.test))) #Presicion SMAPE
(100-ERROR(test$y, as.numeric(y.hat.test))) #Presicion promedio
res1 <- data.frame(y=test$y, y.hat=as.numeric(y.hat.test),
                   time=tiempo[-(1:dim(train)[1])])
p2 <- ggplot(res1, aes(x=time, y=y))+geom_line(color=I('purple')) + theme_minimal()+
  geom_line(data=res1, aes(x=time, y=y.hat),color=I('orange')) +
  ggtitle('Pronostico para 2018,SP800, criterio 80% varianza y PCR')
```

g.  *Grafique la efectividad del pronóstico durante este año.*

```{r graficas1}
p1
p2
```


h.  *¿Cómo mejoraría el pronóstico? si obtiene un promedio en la efectividad mayor al 50% gana puntos extras en proporción a como este valor se acerque al 100% (Puede explorar otros métodos de pronóstico en busca de mayor efectividad, pero siempre contrastando con el criterio de matrices aleatorias).*


En vista de que estamos trabajando con configuraciones de datos de dimensionalidad mediana, consideramos un método de regresión, PLS,que a su vez no requiere de supuestos distribucionales, reduce dimensionalidad y podemos utilizar el criterio de Marchenko Pastur como cota superior para evaluar el número de componentes (a.k.a variables latentes de PLS) y disminuir el tiempo de cómputo. Los resultados mejoran a los anteriores alcanzando una precisión de 85% 



```{r pls, echo=FALSE}
################# resultado con PLS
modelo.pcr.rmt <- plsr(y~., data=train, ncomp=RMT.cota)
#summary(modelo.pcr.rmt)
error <- rep(0, RMT.cota)
for (i in 1:RMT.cota){
y.hat.test <- predict(modelo.pcr.rmt, ncomp=i , newdata = test)
error[i] <- SMAPE(test$y, as.numeric(y.hat.test)) #100-19.15192
}
which.min(error)
modelo.pcr.rmt <- plsr(y~., data=train, ncomp=which.min(error))
y.hat.test <- predict(modelo.pcr.rmt, ncomp=which.min(error) , newdata = test)
print('Presición usando PLS')
(100-SMAPE(test$y, as.numeric(y.hat.test))) #Presicion SMAPE
(100-ERROR(test$y, as.numeric(y.hat.test))) #Presicion promedio
res1 <- data.frame(y=test$y, y.hat=as.numeric(y.hat.test),
                   time=tiempo[-(1:dim(train)[1])])
ggplot(res1, aes(x=time, y=y))+geom_line(color=I('purple')) + theme_minimal()+
  geom_line(data=res1, aes(x=time, y=y.hat),color=I('orange'))+ggtitle('Pronostico en 2018 utilizando PLS y la cota de MK')
```



Finalmente contemplamos la predicción utilizando las series orignales sin ningún preprocesamiento, los resultados son mejores a los anteriores teniendo una presición de 98% usando nuestro criterio,  98.8% de precisión usando 1 componente de PCR y 99% con la estimación con PLS.



```{r braver, echo=FALSE}
# estimar SIN HACER ESTACIONARIAS, ESTIMAR CON SP500 RECORTADO
serie.cruda <- na.omit(serie.cruda)
#View(head(serie.cruda))
colnames(serie.cruda) <- c('time', colnames(serie.cruda)[-1])
colnames(SP500) <- c('time', 'y')
m <- merge(serie.cruda, SP500)
#colnames(m)
tiempo <- m$time 
#tiempo
index <- which(tiempo>ymd('2018-01-01'))
m$time <- NULL
train <- m[-index, ]
test <- m[index,]
vals <- eigen(cor(scale(train[, -dim(train)[2]])))$values
# minisimulacion
set.seed(0)
r <- (dim(train)[2]-1)/dim(train)[1]
x <- c(0,seq((1-r**.5)**2, (1+r**.5)**2, by=0.1), 4)
#plot(x,dmp(x, ndf=dim(train)[1]-1, pdim=dim(train)[2] ), col='purple', type='l')
#rug(vals, col='red')
muestras <- dim(m)[1]*10
set.seed(0)
limite <- mean(rmp(muestras, ndf=dim(train)[1], pdim=dim(train)[2]-1 ))
#abline(v=limite)
vals <- vals[vals>limite]
print('Criterio M-k')
(RMT.cota.2 <- length(vals))
################# resultado con RMT
modelo.pcr.rmt <- pcr(y~., data=train, ncomp=RMT.cota.2)
#summary(modelo.pcr.rmt)
y.hat.test <- predict(modelo.pcr.rmt, ncomp=RMT.cota.2 , newdata = test)
(100-SMAPE(test$y, as.numeric(y.hat.test))) #100- 0.9553613
(100-ERROR(test$y, as.numeric(y.hat.test))) #100- 0.9553613

res1 <- data.frame(y=test$y, y.hat=as.numeric(y.hat.test),
                   time=tiempo[-(1:dim(train)[1])])
library(ggplot2)
ggplot(res1, aes(x=time, y=y))+geom_line(color=I('purple')) + theme_minimal()+
  geom_line(data=res1, aes(x=time, y=y.hat),color=I('orange'))+ ggtitle('Resultado con criterio M-K')
################# resultado con 80 vars
modelo.pcr.rmt <- pcr(y~., data=train, ncomp=dim(train)[2]-1)
#summary(modelo.pcr.rmt)
y.hat.test <- predict(modelo.pcr.rmt, ncomp=1 , newdata = test)
(100-SMAPE(test$y, as.numeric(y.hat.test))) #100- 0.9553613
(100-ERROR(test$y, as.numeric(y.hat.test))) #100- 0.9553613
res1 <- data.frame(y=test$y, y.hat=as.numeric(y.hat.test),
                   time=tiempo[-(1:dim(train)[1])])
ggplot(res1, aes(x=time, y=y))+geom_line(color=I('purple')) + theme_minimal()+
  geom_line(data=res1, aes(x=time, y=y.hat),color=I('orange')) + ggtitle('Resultado con criterio PCR')
################# resultado con PLS
modelo.pcr.rmt <- plsr(y~., data=train, ncomp=RMT.cota.2)
#summary(modelo.pcr.rmt)
error <- rep(0, RMT.cota.2)
for (i in 1:RMT.cota.2){
  y.hat.test <- predict(modelo.pcr.rmt, ncomp=i , newdata = test)
  error[i] <- SMAPE(test$y, as.numeric(y.hat.test)) #100-19.15192
}
print(which.min(error))
y.hat.test <- predict(modelo.pcr.rmt, ncomp=which.min(error) , newdata = test)
(100-SMAPE(test$y, as.numeric(y.hat.test))) #100- 0.9553613
(100-ERROR(test$y, as.numeric(y.hat.test))) #100- 0.9553613

res1 <- data.frame(y=test$y, y.hat=as.numeric(y.hat.test),
                   time=tiempo[-(1:dim(train)[1])])
ggplot(res1, aes(x=time, y=y))+geom_line(color=I('purple')) + theme_minimal()+
  geom_line(data=res1, aes(x=time, y=y.hat),color=I('orange'))+ ggtitle('Resultado con criterio PLS')
```


